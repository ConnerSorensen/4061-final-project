{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 4061 Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2vy8fhqBb8i"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btlWWYHsB4Nw"
      },
      "source": [
        "In this notebook, we will explore computer vision techniques and a simple conceptualization of transfer learning. Our specific goal is to take a dataset of digits (numbers 0-9) and use it to train a model to read speed limit signs. We will try not to touch the training digits dataset, but we will attempt to transform the test signs dataset in order to improve performance. This emulates the situation of having only a pretrained model to work with.\n",
        "\n",
        "This could be very similar to a problem a business faces. For example, Google could face this problem if they were trying to add in speed limit detection to their technology to map out roads. In that situation, the goal would be to take a street image and output what speed limits there are on the street. A huge problem in many DL tasks, but even moreso DL CV tasks, is labeling: labeling is expensive, extremely tedious, hard to standardize, and hard to ensure quality for. It would save lots of money, and perhaps even get a better result, to use existing high quality datasets and models on the new test set. This is the task we will be attempting.\n",
        "\n",
        "The reality of the situation is that transfer learning is an extremely complex field with lots of research, and often turns out to be an unreachable dream, but we will see what we can do by building a model to detect digits and then transform speed limit sign images to improve test accuracy using custom metrics.\n",
        "\n",
        "(To emphasize: **We will be transforming the test set (street sign images) and not the train set (digits)**)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kctlJjCTBd9r"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u5UbagYDP_l"
      },
      "source": [
        "## Training Data\n",
        "\n",
        "The MNIST dataset is often called the \"hello world\" of image classification machine learning. It contains 70,000 examples of handwritten digit images paired with classifications (i.e., the corresponding digit 0-9). It has the right data for our task, but our task is not classification, it is object detection. Therefore, we will use an existing preprocessed derivative called YYMNIST, which generates images with multiple digits each, with a bounding box annotation for each digit. This data is suitable for object detection.\n",
        "\n",
        "The usage of YYMNIST over MNIST corresponds to a functional difference between **classification** and **object detection**: object detection is capable of finding multiple instances in an image (`vector of pixels -> vector of (class, bbox)`) whereas classification is not (`vector of pixels -> single class`) (a good summary of this can be found at https://medium.com/analytics-vidhya/image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81). This is helpful because speed limit signs have two- and three-digit numbers. One alternative not too long ago might have been to use some method to partition the sign image into its comprised digits, but this is no longer necessary and in fact may be much harder than using the highly sophisticated object detection models of today.\n",
        "\n",
        "MNIST source: http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "YYMNIST source: https://github.com/YunYang1994/yymnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_ry1IM7Ghy2"
      },
      "source": [
        "## Test Data\n",
        "Our test data is a subset of a traffic sign dataset that contains many different sign types. We only need the signs that contain a speed limit, though it could be interesting to see if our model has false detections on signs that do not have any numbers. The speed limits we have are: 20, 30, 50, 60, 70, 80, 100, 120.\n",
        "\n",
        "This dataset is from https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign/notebooks. However, it is quite large (~600MB) and we need only a subset, so it seems justifiable to slightly deviate from the normal notebook style and instead chop it outside of the notebook. Note that **this is permitted under the dataset's license** (\"You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission.\"; CC0 1.0). A script of the form below works (copies 30 samples of each relevant sign class; sampling is arguably necessary as there are multiple images of many signs slightly shifted).\n",
        "\n",
        "```\n",
        "import os\n",
        "from random import sample\n",
        "from shutil import copyfile\n",
        "\n",
        "os.makedirs('../speedlimit_signs', exist_ok=True)\n",
        "\n",
        "for cid in range(9):\n",
        "    files = os.listdir(f'Train/{cid}/')\n",
        "    for f in sample(files, 30):\n",
        "        copyfile(f'Train/{cid}/{f}', f'../speedlimit_signs/{f}')\n",
        "```\n",
        "\n",
        "Please download as below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkSytEs-UxCV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K-u0H1CdDH5"
      },
      "source": [
        "You cannot \"Run All\" until this cell has been run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1fk65FWdCWv"
      },
      "source": [
        "try:\n",
        "  import detectron2\n",
        "except:\n",
        "  !pip install pyyaml==5.1\n",
        "  !pip install git+https://github.com/facebookresearch/fvcore.git\n",
        "  import torch, torchvision\n",
        "  assert torch.__version__.startswith(\"1.7\")\n",
        "  !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n",
        "  import os\n",
        "  os.kill(os.getpid(), 9)\n",
        "print('ok')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5W91-t4UzAX"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from random import sample\n",
        "import pandas as pd\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2 import model_zoo\n",
        "from PIL import Image\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bi1jSZvHc0G"
      },
      "source": [
        "### Retrieve Train Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6hEUWPXqV_S"
      },
      "source": [
        "YYMNIST generates the data on the fly via a script and the original MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHUlagVaHcQ3"
      },
      "source": [
        "!git clone https://github.com/YunYang1994/yymnist.git\n",
        "print('Generating train data...')\n",
        "!python yymnist/make_data.py &>/dev/null\n",
        "print('Train data done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M0hRELpLuUz"
      },
      "source": [
        "## Retrieve Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bco8e2B1qbl-"
      },
      "source": [
        "This is our preprocessed signs dataset subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dqENYLzD853"
      },
      "source": [
        "!wget 'https://docs.google.com/uc?export=download&id=1E1-yxHAj-Jvf_NUAfweEMjhlP_YhaJoP' -O speedlimit_signs.zip\n",
        "!unzip -q speedlimit_signs.zip\n",
        "print('Test data done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OkwhPdbRz1O"
      },
      "source": [
        "If everything went well, there should be a folder `speedlimit_signs` and `yymnist/Images`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJvqG06eSBlr"
      },
      "source": [
        "assert set(['speedlimit_signs', 'yymnist']).issubset(os.listdir()),\\\n",
        "'missing top level directory, did you forget to run a cell?'\n",
        "\n",
        "assert 'Images' in os.listdir('yymnist'),\\\n",
        "'missing yymnist images dir, make_data.py must have failed'\n",
        "\n",
        "print('All data is there, excellent.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_GIJXE8S_ik"
      },
      "source": [
        "If those assertions passed, everything should be correct. Let's clean up the filesystem a little bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRI-wQqMTDQ-"
      },
      "source": [
        "!rm *.zip\n",
        "!mkdir test\n",
        "!mv speedlimit_signs/* test\n",
        "!mkdir train\n",
        "!mv yymnist/Images train/\n",
        "!mv yymnist/labels.txt train/labels.txt\n",
        "#!rm -r yymnist/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOzkJDyVVkkd"
      },
      "source": [
        "Now our data structure is like:\n",
        "- train\n",
        "  - Images\n",
        "    - *.jpg\n",
        "  - labels.txt\n",
        "- test\n",
        "  - *.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_35rhJ0Bfsz"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPl7nkePTh52"
      },
      "source": [
        "## Train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Y8QtxkY2WB"
      },
      "source": [
        "Our train data is dynamically generated, so technically that could influence performance significantly, but it is unlikely. Essentially, the repo has the original MNIST digit dataset and generates a bunch of images with the digits randomly overlayed on a white background. The annotations are rectangle bounds for each digit along with the digit it is, i.e. 0-9.\n",
        "\n",
        "The best way to get a feel for this data is to directly visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErfBcSIRZ04B"
      },
      "source": [
        "# partial source: https://github.com/YunYang1994/yymnist/blob/master/show_image.py\n",
        "with open(f'train/labels.txt') as f:\n",
        "  lines = f.readlines()\n",
        "  print(lines)\n",
        "\n",
        "  for x in range(5):\n",
        "    assert lines[x][lines[0].find('.jpg')-1] == f'{x+1}', 'bad labels'\n",
        "    boxes = lines[x][lines[x].find(' ')+1:].split(' ')\n",
        "    img = cv2.imread(f'train/Images/00000{x+1}.jpg')\n",
        "    print(f'train/Images/00000{x+1}.jpg')\n",
        "    for box in boxes:\n",
        "      vals = box.split(',')\n",
        "      img = cv2.rectangle(img, (int(vals[0]), int(vals[1])), (int(vals[2]), int(vals[3])), (0,0,255), 2)\n",
        "    cv2_imshow(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBXOnbEfcVih"
      },
      "source": [
        "It's great (and not so great, later) the annotations are in a simple format. The images look pretty straightforward: a bunch of digits on each image, scaled randomly, positioned randomly.\n",
        "\n",
        "A couple observations, partly from the future:\n",
        "* The bounding boxes are pretty loose. Most likely, the original MNIST images have a bit of edge around them and the script just creates a box around the image. The relevance of this is mainly that since digits can be very close to each other on speed limit signs, this could be bad because it will not have the uniform background it's expecting.\n",
        "* The digits are never rotated at all. This may not matter at all, but it's something to keep in mind.\n",
        "\n",
        "We plan to use the Detectron 2 framework for training a model, and that pretty much requires MS COCO format (basically, a big json dict with everything laid out in a very particular way), so it will be somewhat of a pain to get these annotations to work, but at least we know everything is correct.\n",
        "\n",
        "**The code below deviates from our claim to not modify the training set, but corrects for a bug in yymnist where it generates empty bounding boxes, polluting the training data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djUKVvZyfvKc"
      },
      "source": [
        "print('Processing bounding boxes ...')\n",
        "\n",
        "with open(f'train/labels.txt') as f,\\\n",
        "    open(f'train/labels2.txt', 'w') as nf:\n",
        "  lines = f.readlines()\n",
        "  newlines = []\n",
        "  print(lines)\n",
        "\n",
        "  for x in range(len(lines)):\n",
        "    items = lines[x].split(' ')\n",
        "    newitems = [items[0]]\n",
        "\n",
        "    boxes = items[1:]\n",
        "    #print(boxes)\n",
        "    img = cv2.imread(f'train/Images/{x+1:06d}.jpg')\n",
        "    for box in boxes:\n",
        "      #print(box)\n",
        "      vals = [v.strip() for v in box.split(',')]\n",
        "      newvals = []\n",
        "      #print(vals)\n",
        "      x0 = int(vals[0])\n",
        "      y0 = int(vals[1])\n",
        "      x1 = int(vals[2])\n",
        "      y1 = int(vals[3])\n",
        "      sl = img[y0:y1+1,x0:x1+1]\n",
        "      #cv2_imshow(sl)\n",
        "      gray = cv2.cvtColor(sl, cv2.COLOR_BGR2GRAY)\n",
        "      gray = 255 - gray #cv2.GaussianBlur(255 - gray, , 0)\n",
        "      retval,thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "      mask=Image.fromarray(thresh)\n",
        "      box = mask.getbbox()\n",
        "      if box is None:\n",
        "        #cv2_imshow(sl)\n",
        "        print('Warning: empty box')\n",
        "      else:\n",
        "        newitems.append(','.join(vals))\n",
        "    newlines.append(' '.join(newitems))\n",
        "\n",
        "  nf.write('\\n'.join(newlines))\n",
        "print('Ok done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pFM8Zv7VKEO"
      },
      "source": [
        "## Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJObyu0e9bdo"
      },
      "source": [
        "As stated, the sign dataset has many classes, so we took out 30 random samples from each speed limit class and uploaded the result to drive. Class information can be found at the data source, but superficially, class ids are as follows:\n",
        "* 0 -> 20\n",
        "* 1 -> 30\n",
        "* 2 -> 50\n",
        "* 3 -> 60\n",
        "* 4 -> 70\n",
        "* 5 -> 80\n",
        "* 6 -> 80 (striped)\n",
        "* 7 -> 100\n",
        "* 8 -> 120\n",
        "\n",
        "One detail to note is that since two classes show 80, there are 60x 80 signs and 30x all other speeds. Let's briefly look at a few of each class to get a feel for how these images are. We can also easily verify the counts for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG67xy22WLXR"
      },
      "source": [
        "imgs = os.listdir('test')\n",
        "plt.figure(figsize=(10,10))\n",
        "for x in range(9):\n",
        "  clsimgs = list(filter(lambda i : i.startswith(f'0000{x}'), imgs))\n",
        "  assert len(clsimgs) == 30, 'Unexpected number of images'\n",
        "  for sidx in range(5):\n",
        "    sample=clsimgs[sidx]\n",
        "    plt.subplot(9, 5, x*5 + sidx + 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(cv2.imread(f'test/{sample}')[:,:,::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8I2dFWMXuhG"
      },
      "source": [
        "These signs look pretty bad. Their brightness varies tremendously, many of them are quite blurry, and they are pretty tiny (though that is not visible here). It's hard to predict how the stripe on the 80 signs may affect things, but it seems likely that it would be lower on the striped ones than the normal 80s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcKorIwzB2R-"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIQVLdW63lhl"
      },
      "source": [
        "Detectron 2 is a Python object detection and segmentation framework created by Facebook AI Research (FAIR). We will use Detectron 2 due to familiarity and relative simplicity compared to other DL CV frameworks. The primary difficulty with it is getting our data in the right format, since Detectron 2 mostly uses MS COCO, the format of the datased named COCO (Common Objects in COntext)\n",
        "\n",
        "**Note, for full clarity, that this is the only distinct \"model\" we will use throughout the notebook.** Specifically what we are doing is training a Detectron 2 model on YYMNIST images, and then optimizing its performance on a street sign test dataset using custom metrics and various transformations.\n",
        "\n",
        "The specific type of model we will use is called Faster R-CNN (the CNN being the same CNN we have discussed in class). It is a very recent and extraordinarily powerful object detection model that has achieved great performance, both in speed and accuracy, in many areas in the past few years. It works by extracting image features using a CNN architecture and then making region proposals over the features. These \"proposals\" are potential object detections which are associated with a probability. For example, the model could output a bounding box over a dog that means, \"the model is ~85% confient this bounding box is a dog.\" Ideally, all detections would be have 100% probability/confidence, but this is far from the case in practice. If confidence is too low, then you get tons of garbage detections, and if it is too high, then you filter out too many real detections. We will require detections to have 60%, as we experimentally determined.\n",
        "\n",
        "60% is low but this is logical as a YYMNIST object detection model is inherently extremely overfit in the context of evaluation on a street sign dataset.\n",
        "\n",
        "The internals of our object detection model are not the focus of this notebook, but it is nonetheless difficult to convey rationale behind what is going on without giving a slight background. A good overview can be found at https://towardsdatascience.com/faster-rcnn-object-detection-f865e5ed7fc4.\n",
        "\n",
        "The code below reads the labels2.txt we got earlier and transforms it into MS COCO format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu1ZhU3_4EJp"
      },
      "source": [
        "cocoDict = {\n",
        "    'info': {\n",
        "        'description': 'YYMNIST Generated Dataset',\n",
        "        'url': 'https://github.com/YunYang1994/yymnist',\n",
        "        'version': '1.0',\n",
        "        'year': 2020\n",
        "    },\n",
        "    \n",
        "    'images': [],\n",
        "    'annotations': [],\n",
        "    'categories': [\n",
        "                   { 'id': x, 'name': str(x) } for x in range(10)\n",
        "    ]\n",
        "}\n",
        "\n",
        "imgIdCounter = 0\n",
        "annoIdCounter = 0\n",
        "\n",
        "with open('train/labels2.txt', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    blocks = line.split(' ')\n",
        "    imgfn = blocks[0]\n",
        "    imgfn = imgfn[imgfn.rfind('/')+1:] # /content/yymnist/Images/x -> x\n",
        "\n",
        "    # insert image\n",
        "    cocoDict['images'].append({\n",
        "        'id': imgIdCounter,\n",
        "        'width': 416,\n",
        "        'height': 416,\n",
        "        'file_name': imgfn\n",
        "    })\n",
        "\n",
        "    for anno in blocks[1:]:\n",
        "      parts = anno.split(',')\n",
        "      cocoDict['annotations'].append({\n",
        "          'id': annoIdCounter,\n",
        "          'category_id': int(parts[-1]),\n",
        "          'image_id': imgIdCounter,\n",
        "          'bbox': [\n",
        "                   int(parts[0]),\n",
        "                   int(parts[1]),\n",
        "                   int(parts[2]) - int(parts[0]),\n",
        "                   int(parts[3]) - int(parts[1])\n",
        "          ]\n",
        "      })\n",
        "      annoIdCounter += 1\n",
        "\n",
        "    imgIdCounter += 1\n",
        "\n",
        "with open('train/labels.json', 'w') as fo:\n",
        "  json.dump(cocoDict, fo)\n",
        "\n",
        "assert len(cocoDict['categories']) == 10, 'Something went horribly wrong'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OFPdxF0ePf2"
      },
      "source": [
        "Detectron works by defining a COCO dataset and then \"registering\" it in its internal database so that it can then be referenced by name as a train or test dataset for a model. The utility of this is that it makes things much more portable, but it's not of much concern for this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaidDP3DAjB2"
      },
      "source": [
        "register_coco_instances('yymnist', {}, 'train/labels.json', 'train/Images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSkWOfh5GcL8"
      },
      "source": [
        "Now for training. After some experimentation, 2000 iterations seems like a pretty good training number, although we did not determine this with proper training curves.\n",
        "\n",
        "We trained a model this long and uploaded it to drive, but a new one can be trained easily.\n",
        "\n",
        "Credit to\n",
        "* https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=Ya5nEuMELeq8 (official detectron 2; though tutorial uses mask r-cnn)\n",
        "* https://www.flagly.org/projects/4/notes/41/sections/41/\n",
        "\n",
        "(2000 iterations takes something like 22 minutes, so it's not that long, but it's far too long to wait in a presentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEQs072_cW4Y"
      },
      "source": [
        "## Download or train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VkJVm2TOxrF"
      },
      "source": [
        "Models work using a yaml structure containing parameters, which you can then train or evaluate with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBF22O7tqDCS"
      },
      "source": [
        "train_new = False\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"yymnist\",)\n",
        "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.02\n",
        "cfg.SOLVER.MAX_ITER = 3000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10\n",
        "\n",
        "if not train_new:\n",
        "  !gdown https://drive.google.com/uc?id=1Esa1vz7JCS4K1JDZdvIv0feIL2sJGDjd\n",
        "  !mkdir output\n",
        "  !mv model_final.pth output\n",
        "  cfg.MODEL.WEIGHTS = \"output/model_final.pth\"\n",
        "  print('Loaded')\n",
        "else:\n",
        "  cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "  os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "  trainer = DefaultTrainer(cfg)\n",
        "  trainer.resume_or_load(resume=False)\n",
        "  trainer.train()\n",
        "  print('Trained')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjbVm01TIxAX"
      },
      "source": [
        "Let's do a little sanity check (note this is NOT \"testing\", this is running the model on a **train** image. However, if this looks OK then it means everything is in the right format and the training went reasonably OK).\n",
        "\n",
        "The metadata is something we could have registered properly with the dataset, but it doesn't really matter. We are just using it to make Detectron label the bounding boxes so we don't have to manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS7R3O7sCYP0"
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n",
        "predictor = DefaultPredictor(cfg)\n",
        "im = cv2.imread('train/Images/000001.jpg')\n",
        "eval_metadata = { 'thing_classes': [str(x) for x in range(10)] } # might have been better to register the dataset with this but doesn't really matter\n",
        "\n",
        "def detect_image(im):\n",
        "  # weird indexing notation is for RGB vs BGR\n",
        "  v = Visualizer(im[:,:,::-1], eval_metadata, scale=1.5)\n",
        "  outputs = predictor(im)\n",
        "  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "  return outputs['instances'], out.get_image()[:, :, ::-1]\n",
        "\n",
        "instances, img = detect_image(im)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aISVZnNFJl-f"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Now let's try evaluation on street signs. One caveat is that we lack a precise way of evaluation predictions, because we do not have bounding box labels for the street sign. One option, which we will do, is to take the expected speed limit based on the sign image's class and match it to the bounding boxes. Intuitively, if you have a '5' box to the left of a '0' box, then that represents '50', and if the sign is class 2 (i.e. 50 kph), then it was correct.\n",
        "\n",
        "But there are problems with this. For one, any extraneous bounding boxes result in an incorrect result. For example, if there is a '2' detection outside of the sign (for whatever reason; it could be erroneous or there could be another sign or something) then the result might be '250' which is completely incorrect. In the other direction, it's possible to be correct when the prediction was really incorrect. For example, a sign of 100 kph could have the first 0 double-detected and the second 0 not detected and that would result in a false correct prediction.\n",
        "\n",
        "However, it suffices here. As we will be able to see by eye, the vast majority of the time this metric says \"correct\", it really is correct.\n",
        "\n",
        "One thing to note is that even if we did have bounding box annotations for the stop signs (though that would invalidate this whole scenario), it still would not necessarily be straightforward to evaluate, but we could use something like the Hungarian algorithm to match boxes for a slightly more strict metric. Unfortunately, this is far outside of the scope of this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDFy0_T6JpL0"
      },
      "source": [
        "speed_map = {\n",
        "    '0': '20',\n",
        "    '1': '30',\n",
        "    '2': '50',\n",
        "    '3': '60',\n",
        "    '4': '70',\n",
        "    '5': '80',\n",
        "    '6': '80',\n",
        "    '7': '100',\n",
        "    '8': '120'\n",
        "}\n",
        "\n",
        "results_dict = {}\n",
        "\n",
        "def eval_transform(tname, transform):\n",
        "  imgs = os.listdir('test')\n",
        "\n",
        "  num_correct = 0\n",
        "  num_incorrect = 0\n",
        "  num_nodet = 0\n",
        "\n",
        "  for x in imgs:\n",
        "    answer = speed_map[x[4]]\n",
        "    im = transform(cv2.imread(f'test/{x}'))\n",
        "    instances, img = detect_image(im)\n",
        "    boxes = [x.cpu().numpy() for x in list(instances.get('pred_boxes'))]\n",
        "    classes = [int(x.cpu().numpy()) for x in list(instances.get('pred_classes'))]\n",
        "    if len(boxes) > 0:\n",
        "      # https://stackoverflow.com/questions/6618515/sorting-list-based-on-values-from-another-list\n",
        "      # sort detected classes by left edge, and then concat the digits\n",
        "      # detected_num is the number the model things is on the sign\n",
        "      detected_num = ''.join([str(x) for _,x in sorted(zip(boxes, classes), key=lambda pair : pair[0][0])])\n",
        "      if detected_num == answer:\n",
        "        print(f'{answer}... correct!')\n",
        "        num_correct += 1\n",
        "      else:\n",
        "        print(f'{answer}... incorrect ({detected_num})!')\n",
        "        num_incorrect += 1\n",
        "      cv2_imshow(img)\n",
        "    else:\n",
        "      #print(f'{answer}... no detections!')\n",
        "      num_nodet += 1\n",
        "\n",
        "  print(f'{num_correct} correct')\n",
        "  print(f'{num_incorrect} incorrect')\n",
        "  print(f'{num_nodet} no detections')\n",
        "  results_dict[tname] = [num_correct/len(imgs), num_incorrect/len(imgs), num_nodet/len(imgs)]\n",
        "  return results_dict[tname]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs3o9iiaiELG"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in3a6jtPZgwc"
      },
      "source": [
        "# Evaluate images w/o changing them\n",
        "eval_transform('Baseline', lambda x:x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCfoWjtizs8"
      },
      "source": [
        "We have a 6.3% accuracy.\n",
        "\n",
        "This isn't too bad because the chance of accidental correctness is very low. It's not guessing which class the sign is among 8; it's more like guessing the right number from 0 to 1000 (since the model can guess an arbitrary number of digits).\n",
        "\n",
        "Also, many of the \"incorrect\" ones are partially correct. Commonly, the 0 gets detected but not the other digit. We suspect this has to do with the digits being close enough together that the overly loose YMNIST boxes see parts of the other digits in the boxes for each digit, making them unrecognizable.\n",
        "\n",
        "We did a separate experiment using OpenCV thresholds to fully tighten the bounding boxes on the digits, but shockingly, this caused accuracy to drop further. New classes of errors were introduced such as two 0s being detected on the two circles of an 8. This is logically most likely because some distinct background is necessary context to determine where a digit starts and where it stops. Even now, some errors include things like a 0 being detected on the circle of a 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEFiXxppiIB1"
      },
      "source": [
        "### Black & white + Resize\n",
        "(better than either individually)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-3zZVxpZjBA"
      },
      "source": [
        "# turn to grayscale and resize\n",
        "def grayscale_resize_transform(im):\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "  im2 = np.zeros_like(im)\n",
        "  im2[:,:,0] = gray\n",
        "  im2[:,:,1] = gray\n",
        "  im2[:,:,2] = gray\n",
        "  return cv2.resize(im2, (200,200))\n",
        "\n",
        "eval_transform('Grayscale+Resize', grayscale_resize_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUd-9SMtjtBD"
      },
      "source": [
        "By making the images black and white, accuracy went up slightly to 6.7%. It makes sense that black and white training data does not work well with RGB test data. It seems surprising \"incorrect\" (aka, often partially correct) went down because you'd think when the test data becomes more similar to the training data there would be more detections in general, even in the background, but that does not seem to be the case here. One can only really guess at the cause of this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIOgZBx93xv1"
      },
      "source": [
        "###Contrast\n",
        "\n",
        "#### Summarized\n",
        "We tried several combinations and orderings of transformations involving contrast. Combining contrast boosting, grayscale, and image resizing by first resizing, then grayscaling, then contrast boosting the test images, ended up producing the best results with our model at almost double baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhpZC9JQ5dLA"
      },
      "source": [
        "####Contrast Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOnI3meU8eYx"
      },
      "source": [
        "Pure contrast boosting seems to perform poorly on its own, further image transformations will be tested. The intuition behind contrast boosting is that it could make the number lines more distinctive compared to the background, as more prominent features are detected more easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XEBIvoF3wtu"
      },
      "source": [
        "import cv2\n",
        "def contrast_boost(im): # from https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv\n",
        "  #-----Reading the image-----------------------------------------------------\n",
        "  # images are already in correct format\n",
        "  # img = cv2.imread(im, 1)\n",
        "\n",
        "  #-----Converting image to LAB Color model----------------------------------- \n",
        "  lab= cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "  #-----Splitting the LAB image to different channels-------------------------\n",
        "  l, a, b = cv2.split(lab)\n",
        "\n",
        "  #-----Applying CLAHE to L-channel-------------------------------------------\n",
        "  clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "  cl = clahe.apply(l)\n",
        "\n",
        "  #-----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
        "  limg = cv2.merge((cl,a,b))\n",
        "\n",
        "  #-----Converting image from LAB Color model to RGB model--------------------\n",
        "  final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "  #_____END_____#\n",
        "  return final\n",
        "\n",
        "print(eval_transform('Contrast', contrast_boost))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I3N9uxpkaKC"
      },
      "source": [
        "Interestingly, this had the exact same result (on RGB) as going RGB -> B&W. This is most likely a coincidence, but the result is the same: small effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrcjqGJE5X1Y"
      },
      "source": [
        "####Contrast Boost and Resize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aEop4I06jLw"
      },
      "source": [
        "Attempting resizing first and second, it appears that resizing the image first, then boosting the contrast produces a better result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l3qyKBb5nU8"
      },
      "source": [
        "def contrast_resize(im):\n",
        "  return cv2.resize(contrast_boost(im), (200, 200))\n",
        "\n",
        "def resize_contrast(im):\n",
        "  return contrast_boost(cv2.resize(im, (200, 200)))\n",
        "\n",
        "print(eval_transform('Resize+Contrast', resize_contrast))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUvDDebJlRjc"
      },
      "source": [
        "10% accuracy is getting pretty great in the grand scheme of things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjsTlC3T60_M"
      },
      "source": [
        "####Contrast, Resize, and Grayscale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpHcnc8C7zJf"
      },
      "source": [
        "From these tests, resizing first, then turning the image into grayscale, then boosting the contrast produced the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP_fGqIn64U_"
      },
      "source": [
        "def resize_contrast_grayscale(im):\n",
        "  return grayscale_resize_transform(contrast_boost(cv2.resize(im, (200, 200))))\n",
        "\n",
        "def resize_grayscale_contrast(im):\n",
        "  return contrast_boost(grayscale_resize_transform(cv2.resize(im, (200, 200))))\n",
        "\n",
        "eval_transform('Resize+Grayscale+Contrast', resize_grayscale_contrast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEGM6mMBln9M"
      },
      "source": [
        "Accuracy went up just a tiny bit more; while it doesn't entirely make intuitive sense, it seems the utility from constrast boosting and B&W overlaps a fair bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdBkr-NA8Ea6"
      },
      "source": [
        "###Edge Detection\n",
        "\n",
        "Edge detection ended up resulting in very poor performance, but very high detection rates. Unfortunately what got detected was not necessarily the numbers on the signs. Our hypothesis is that edge detection made background equally as prominent as the digits on the sign, and thus confused the model. This also revealed a secondary flaw in our model, that it cannot weed out false detections. For instance there is no way a sign could have more than 3 digits, but the model will happily detect far more digits than that.\n",
        "\n",
        "One can also see that the edges for the speed limit digits generally trace around the number's text on both the inside and outside, so most of the \"filled\" part is actually white, just like the background. While performance is not great here, it reduces so much noise while still keeping valuable digit features so we suspect it has the potential to help a lot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX2A92apAfn9"
      },
      "source": [
        "####Canny Edge Detection and Inverted Canny Edge Detection\n",
        "Both edge detection and inverted edge detection appear to bring very high detection rates, but thus far no correct detections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBrfu1ur8GWy"
      },
      "source": [
        "def edges(im):\n",
        "  edges = cv2.Canny(im, 100, 200)\n",
        "  im2 = np.zeros_like(im)\n",
        "  im2[:,:,0] = edges\n",
        "  im2[:,:,1] = edges\n",
        "  im2[:,:,2] = edges\n",
        "  return im2\n",
        "\n",
        "def invert(im):\n",
        "  return cv2.bitwise_not(im)\n",
        "\n",
        "def inverse_edges(im):\n",
        "  return invert(edges(im))\n",
        "\n",
        "eval_transform('Inverse Edges', inverse_edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2dva34mAnbV"
      },
      "source": [
        "####Resizing and Canny Edge Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aB_LpMKmuqY"
      },
      "source": [
        "Now we add in resizing to the mix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6AZRXuUA489"
      },
      "source": [
        "def edge_resize(im):\n",
        "  return cv2.resize(edges(im), (200, 200))\n",
        "\n",
        "def resize_edge(im):\n",
        "  return edges(cv2.resize(im, (200, 200)))\n",
        "\n",
        "def inverse_edge_resize(im):\n",
        "  return invert(edge_resize(im))\n",
        "\n",
        "def inverse_resize_edge(im):\n",
        "  return invert(resize_edge(im))\n",
        "\n",
        "eval_transform('Inverse Resize Edge', inverse_resize_edge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC92nXHmmyNF"
      },
      "source": [
        "It is interesting how destructive this small change is to the number of lines in the images. Intuitively, upscaling an image makes everything thicker, so perhaps many lines lose their qualification as edges due to being too thick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_sgrWAgEPMk"
      },
      "source": [
        "####Automatic Parameter Tuning Canny Edge Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0POeY0gEOc3"
      },
      "source": [
        "# from https://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/\n",
        "def auto_canny(image, sigma=0.33):\n",
        "  # compute the median of the single channel pixel intensities\n",
        "  v = np.median(image)\n",
        "  # apply automatic Canny edge detection using the computed median\n",
        "  lower = int(max(0, (1.0 - sigma) * v))\n",
        "  upper = int(min(255, (1.0 + sigma) * v))\n",
        "  edged = cv2.Canny(image, lower, upper)\n",
        "  # return the edged image\n",
        "  im2 = np.zeros_like(image)\n",
        "  im2[:,:,0] = edged\n",
        "  im2[:,:,1] = edged\n",
        "  im2[:,:,2] = edged\n",
        "  return im2\n",
        "\n",
        "def inverse_auto_canny(im):\n",
        "  return invert(auto_canny(im))\n",
        "\n",
        "eval_transform('Auto Canny', inverse_auto_canny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLaEdvu9nH5s"
      },
      "source": [
        "Still no luck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVpGMIGeFqdO"
      },
      "source": [
        "Let's try with resizing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqWiHlSKFtj1"
      },
      "source": [
        "def resize_auto_canny(im):\n",
        "  return auto_canny(cv2.resize(im, (200, 200)))\n",
        "\n",
        "def auto_canny_resize(im):\n",
        "  return cv2.resize(auto_canny(im), (200, 200))\n",
        "\n",
        "def inverse_resize_auto_canny(im):\n",
        "  return invert(resize_auto_canny(im))\n",
        "\n",
        "def inverse_auto_canny_resize(im):\n",
        "  return invert(auto_canny_resize(im))\n",
        "\n",
        "eval_transform('Resizing Auto Canny', inverse_resize_auto_canny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nJMNw_bna8t"
      },
      "source": [
        "In spite of all of the 0s, we still believe canny has potential, but it is clear that simple transformations like inversion, resizing, etc. will not enable compatibility with the model. Ultimately, some specialized algorithm is likely needed to either reinforce the digits or get rid of some of the background.\n",
        "\n",
        "Even if it's not useful for being used with a regular MNIST model due to the simplicity yet lack of feature loss, it would probably be productive to train a model on edge-detected sign images over regular sign images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1spipz09Yeu7"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8indAGVKe_7"
      },
      "source": [
        "Evaluation metrics are complex here because it is hard to define correctness.\n",
        "* A prediction of \"150\" for 50 means there was probably an extraneous 1 somewhere.\n",
        "* A prediction of \"5\" for 50 means the 5 got detected but not the 0.\n",
        "* A prediction of \"1234\" for 50 means it was completely wrong.\n",
        "\n",
        "Yet, all of these cases are grouped together: there is a prediction but it is incorrect. This must be kept in mind; except for the edge detection evaluations, most incorrect predictions were partially correct actually. Clearly, though, all \"no detection\" cases are fully incorrect.\n",
        "\n",
        "It would be possible to add heuristics to the accuracy metric to infer the real correct score. For example, the largest (up to) 3 boxes could be considered to eliminate some of the extraneous small boxes. We did not do this because it opens up a difficult balancing act."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Iel6Qknpwh"
      },
      "source": [
        " df = pd.DataFrame(results_dict, index=['Accuracy', 'Incorrect', 'No prediction'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9k7tJyYKbRw"
      },
      "source": [
        "One thing we will add is a pseudo-recall measuring \"when the model predicted something, how often was it completely correct?\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhgRAlLzJz4N"
      },
      "source": [
        "df.loc['Recall'] = df.loc['Accuracy'] / (df.loc['Accuracy'] + df.loc['Incorrect'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4fjbdntKi3f"
      },
      "source": [
        "Now we can plot. We will separate regular and edge/canny transformations to reduce clutter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrfKdc6hKwcM"
      },
      "source": [
        "### Non-Edge Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng_3qTWkKlFi"
      },
      "source": [
        "df.iloc[:,:5].plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBBHDQcLLQ4n"
      },
      "source": [
        "Resize + Grayscale + Contrast Boost is the winner here. While its accuracy is only slightly over 10%, its \"recall\" is around 25% and it has one of the lower \"no prediction\" rates. This is far from great, but given that most of \"Incorrect\" is partially correct (or correct w/ extraneous digits), it's not actually that bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4lsxed2LmN8"
      },
      "source": [
        "### Edge Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_vzFoqnLoRX"
      },
      "source": [
        "Edge transformations gave very poor objective accuracy, but their \"Incorrect\" vs \"No prediction\" rates did vary significantly. Part of this is because of the destructive effect pre-resizing has on the edges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qw6mmHBLlv4"
      },
      "source": [
        "df.iloc[:,5:].plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJNsCZc1L5Sw"
      },
      "source": [
        "Resizing Auto Canny would likely be the one we would look more into because high \"Incorrect\" as opposed to \"No prediction\" means there are at least some features being detected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvDCnExJlrGv"
      },
      "source": [
        "# Conclusion\n",
        "In this notebook, we explored computer vision as a means of experimenting with primitive ideas of transfer learning. We took a simple handwritten dataset, and trained a model to try to read speed limit signs of varying quality, brightness, and other traits.\n",
        "\n",
        "We started at 6.3% accuracy, but through choosing and applying image transformations to the signs, we achieved 10.4% overall accuracy. This is low, but a big improvement and still far higher than randomly guessing a number.\n",
        "\n",
        "The most natural next step for this project would be to perform **input augmentation**, i.e. modifying the training images rather than just the test images. This can be tricky and increases training time a lot, but things like extremely dark and messed up signs are really hard to clean whereas similar destruction could be applied to the training set and the model could possibly learn how to read them. This does break part of our scenario.\n",
        "\n",
        "Another possible consideration would be using a different training set altogether. It is possible that the handwritten digits in the yymnist dataset caused the model to be poorly trained for the very uniform digits on the road signs. Perhaps a training dataset made up of images of digits in many fonts would produce better results. On the other hand the irregularities of the yymnist dataset could also have produced a better model, because the model might have been better prepared to handle skewed or rotated images by having broader criteria for what each digit might look like.\n",
        "\n",
        "Overall, this was a really fun project that has near infinite room for further improvement. We learned a lot about computer vision (specifically object detection) and image transformations (both generally and OpenCV functionality), and we scratched the surface of transfer learning. Given our poor overall accuracy at the end, it becomes clear why this is a difficult area with tons of research and specialized techniques of its own."
      ]
    }
  ]
}